#!/usr/bin/env python
"""
Fix for the train/test split issue in strategy state management.

This script demonstrates the issue where strategy instances maintain internal state
across train and test runs, resulting in incorrect optimization results.
"""

import logging
import os
import sys
import numpy as np
import pandas as pd
from datetime import datetime

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def run_direct_test():
    """Run a direct test to verify the strategy state issue and fix."""
    logger.info("Starting direct test of train/test isolation...")
    
    # Create test data - simulated price series
    dates = pd.date_range(start='2025-01-01', periods=200, freq='D')
    prices = np.linspace(100, 150, 200) + np.sin(np.linspace(0, 10, 200)) * 20
    
    df = pd.DataFrame({
        'timestamp': dates,
        'close': prices,
        'open': prices - 1,
        'high': prices + 1,
        'low': prices - 2,
        'volume': 1000000 * np.ones(len(prices))
    })
    
    # Create a simple train/test split - 70/30
    split_idx = int(len(df) * 0.7)
    
    # Create deep copies of the data to ensure isolation
    train_df = df.iloc[:split_idx].copy(deep=True)
    test_df = df.iloc[split_idx:].copy(deep=True)
    
    # Reset indices to avoid any shared state
    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)
    
    # Create symbol name
    symbol = 'SIM'
    
    # Add symbol column to identify the data
    train_df['symbol'] = symbol
    test_df['symbol'] = symbol
    
    # Create event emulation for backtest
    class TestEventBus:
        def __init__(self):
            self.handlers = {}
            
        def subscribe(self, event_type, handler):
            if event_type not in self.handlers:
                self.handlers[event_type] = []
            self.handlers[event_type].append(handler)
            
        def publish(self, event):
            event_type = event.type
            if event_type in self.handlers:
                for handler in self.handlers[event_type]:
                    handler(event)
                    
    class TestEvent:
        def __init__(self, type, data):
            self.type = type
            self.data = data
            
        def get_data(self):
            return self.data
    
    # Create event types
    class EventType:
        BAR = "BAR"
        SIGNAL = "SIGNAL"
        FILL = "FILL"
        PORTFOLIO_UPDATE = "PORTFOLIO_UPDATE"
    
    # Create the strategy - using a factory-like approach
    def create_strategy_instance(name, fast_period=10, slow_period=30):
        """Create a completely fresh strategy instance."""
        
        # Import the strategy class
        from src.strategy.implementations.simple_ma_crossover import SimpleMACrossoverStrategy
        
        # Create a new strategy instance
        strategy = SimpleMACrossoverStrategy(
            name=name,
            fast_period=fast_period,
            slow_period=slow_period,
            position_size=100
        )
        
        # Log created instance memory address for debugging
        logger.info(f"Created strategy instance at {hex(id(strategy))}")
        logger.info(f"Strategy params: fast_period={fast_period}, slow_period={slow_period}")
        
        return strategy
    
    # Function to run a backtest on a dataset
    def run_backtest(df, strategy, log_prefix=""):
        """Run a simple backtest using the provided dataframe and strategy."""
        
        # Create event bus
        event_bus = TestEventBus()
        
        # Initialize strategy
        strategy.event_bus = event_bus
        strategy.event_bus.subscribe(EventType.BAR, strategy.on_bar)
        
        # Reset strategy state to ensure clean run
        logger.info(f"{log_prefix} Explicitly resetting strategy state before run")
        strategy.reset()
        
        # Log strategy state before run
        logger.info(f"{log_prefix} Strategy prices dict size before: {len(strategy.prices)}")
        
        # Capture signals generated by strategy
        signals = []
        
        def on_signal(event):
            signal_data = event.get_data()
            signals.append(signal_data)
            logger.info(f"{log_prefix} Signal received: {signal_data['direction']} at price {signal_data['price']:.2f}")
            
        event_bus.subscribe(EventType.SIGNAL, on_signal)
        
        # Process each bar
        for i, row in df.iterrows():
            # Create bar event
            bar_data = {
                'symbol': row['symbol'],
                'open': row['open'],
                'high': row['high'],
                'low': row['low'],
                'close': row['close'],
                'volume': row['volume'],
                'timestamp': row['timestamp']
            }
            
            # Send bar event to strategy
            event = TestEvent(EventType.BAR, bar_data)
            event_bus.publish(event)
            
        # Calculate metrics
        metrics = {
            'total_signals': len(signals),
            'long_signals': len([s for s in signals if s['direction'] == 'LONG']),
            'short_signals': len([s for s in signals if s['direction'] == 'SHORT']),
            'total_return': 0.0,  # Would calculate actual returns in real backtest
            'sharpe_ratio': 0.0,  # Would calculate in real backtest
            'prices_length': len(strategy.prices.get(symbol, []))
        }
        
        # Simulate a PnL calculation based on signals
        # This is just a simple approximation for testing
        if signals:
            price_df = df.copy()
            price_df['direction'] = 0
            
            for signal in signals:
                idx = price_df[price_df['timestamp'] >= signal['timestamp']].index[0]
                if signal['direction'] == 'LONG':
                    price_df.loc[idx:, 'direction'] = 1
                else:
                    price_df.loc[idx:, 'direction'] = -1
                    
            # Calculate returns based on directional positions
            price_df['returns'] = price_df['close'].pct_change() * price_df['direction'].shift(1)
            price_df['equity'] = (1 + price_df['returns']).cumprod()
            
            metrics['total_return'] = price_df['equity'].iloc[-1] - 1 if not price_df['equity'].empty else 0
            
            # Calculate sharpe ratio
            if len(price_df['returns']) > 1:
                metrics['sharpe_ratio'] = price_df['returns'].mean() / price_df['returns'].std() * np.sqrt(252) if price_df['returns'].std() > 0 else 0
        
        # Log strategy state after run
        logger.info(f"{log_prefix} Strategy prices dict size after: {len(strategy.prices)}")
        if symbol in strategy.prices:
            logger.info(f"{log_prefix} Strategy prices array length for {symbol}: {len(strategy.prices[symbol])}")
        
        return metrics
    
    # PROBLEM DEMONSTRATION: Create one strategy instance and use it for both train and test
    # This simulates the issue where the same strategy instance is reused during optimization
    logger.info("\n===== PROBLEM DEMONSTRATION (REUSING STRATEGY INSTANCE) =====")
    shared_strategy = create_strategy_instance("shared_strategy", fast_period=10, slow_period=30)
    
    logger.info("\nRunning train backtest with shared strategy...")
    train_metrics = run_backtest(train_df, shared_strategy, "TRAIN:")
    
    logger.info("\nRunning test backtest with same shared strategy instance...")
    test_metrics = run_backtest(test_df, shared_strategy, "TEST:")
    
    # Compare results
    logger.info("\nCOMPARISON OF RESULTS WITH SHARED STRATEGY INSTANCE:")
    logger.info(f"Train metrics: {train_metrics}")
    logger.info(f"Test metrics: {test_metrics}")
    
    logger.info(f"Train prices length: {train_metrics['prices_length']}")
    logger.info(f"Test prices length: {test_metrics['prices_length']}")
    
    logger.info("Issue detection: Are the test's price array sizes suspiciously large?")
    if test_metrics['prices_length'] > len(test_df):
        logger.error("CONFIRMED ISSUE: Test prices array is larger than the test dataframe!")
        logger.error(f"This indicates state is being carried over from train to test!")
    
    # SOLUTION: Create separate strategy instances for train and test
    logger.info("\n===== SOLUTION DEMONSTRATION (FRESH STRATEGY INSTANCES) =====")
    
    logger.info("\nRunning train backtest with new strategy...")
    train_strategy = create_strategy_instance("train_strategy", fast_period=10, slow_period=30)
    train_metrics_fixed = run_backtest(train_df, train_strategy, "TRAIN:")
    
    logger.info("\nRunning test backtest with new separate strategy...")
    test_strategy = create_strategy_instance("test_strategy", fast_period=10, slow_period=30)
    test_metrics_fixed = run_backtest(test_df, test_strategy, "TEST:")
    
    # Compare results
    logger.info("\nCOMPARISON OF RESULTS WITH SEPARATE STRATEGY INSTANCES:")
    logger.info(f"Train metrics: {train_metrics_fixed}")
    logger.info(f"Test metrics: {test_metrics_fixed}")
    
    logger.info(f"Train prices length: {train_metrics_fixed['prices_length']}")
    logger.info(f"Test prices length: {test_metrics_fixed['prices_length']}")
    
    logger.info("Solution validation: Are the test's price array sizes consistent with the test data?")
    if test_metrics_fixed['prices_length'] <= len(test_df):
        logger.info("CONFIRMATION: Test prices array size is consistent with test data!")
    
    # Verify changes in results between problematic and fixed approach
    logger.info("\nIMPACT OF THE FIX:")
    
    # Determine if the train metrics changed
    train_metrics_changed = (train_metrics['total_signals'] != train_metrics_fixed['total_signals'] or
                            abs(train_metrics['total_return'] - train_metrics_fixed['total_return']) > 0.001)
    
    # Determine if the test metrics changed
    test_metrics_changed = (test_metrics['total_signals'] != test_metrics_fixed['total_signals'] or
                           abs(test_metrics['total_return'] - test_metrics_fixed['total_return']) > 0.001)
    
    logger.info(f"Train metrics changed: {train_metrics_changed}")
    logger.info(f"Test metrics changed: {test_metrics_changed}")
    
    # Show more detailed comparison if changes detected
    if train_metrics_changed or test_metrics_changed:
        logger.info("\nDetailed comparison of metrics:")
        
        if train_metrics_changed:
            logger.info("\nTRAIN METRICS COMPARISON:")
            logger.info(f"  Original signals: {train_metrics['total_signals']}, Fixed signals: {train_metrics_fixed['total_signals']}")
            logger.info(f"  Original return: {train_metrics['total_return']:.4f}, Fixed return: {train_metrics_fixed['total_return']:.4f}")
            
        if test_metrics_changed:
            logger.info("\nTEST METRICS COMPARISON:")
            logger.info(f"  Original signals: {test_metrics['total_signals']}, Fixed signals: {test_metrics_fixed['total_signals']}")
            logger.info(f"  Original return: {test_metrics['total_return']:.4f}, Fixed return: {test_metrics_fixed['total_return']:.4f}")
    
    # Return a success indicator
    return not (train_metrics_changed or test_metrics_changed)

if __name__ == "__main__":
    run_direct_test()